{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"8\"\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "from llavavid.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llavavid.conversation import conv_templates, SeparatorStyle\n",
    "from llavavid.model.builder import load_pretrained_model\n",
    "from llavavid.utils import disable_torch_init\n",
    "from llavavid.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n",
    "\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from decord import VideoReader, cpu\n",
    "\n",
    "from transformers import AutoConfig\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def split_list(lst, n):\n",
    "    \"\"\"Split a list into n (roughly) equal-sized chunks\"\"\"\n",
    "    chunk_size = math.ceil(len(lst) / n)  # integer division\n",
    "    return [lst[i: i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "\n",
    "def get_chunk(lst, n, k):\n",
    "    chunks = split_list(lst, n)\n",
    "    return chunks[k]\n",
    "\n",
    "\n",
    "def parse_args(args=None):\n",
    "    \"\"\"\n",
    "    Parse command-line arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Define the command-line arguments\n",
    "    parser.add_argument(\"--video_path\", help=\"Path to the video files.\", required=True)\n",
    "    parser.add_argument(\"--output_dir\", help=\"Directory to save the model results JSON.\", required=True)\n",
    "    parser.add_argument(\"--output_name\", help=\"Name of the file for storing results JSON.\", required=True)\n",
    "    parser.add_argument(\"--model-path\", type=str, default=\"facebook/opt-350m\")\n",
    "    parser.add_argument(\"--model-base\", type=str, default=None)\n",
    "    parser.add_argument(\"--conv-mode\", type=str, default=None)\n",
    "    parser.add_argument(\"--chunk-idx\", type=int, default=0)\n",
    "    parser.add_argument(\"--mm_resampler_type\", type=str, default=\"spatial_pool\")\n",
    "    parser.add_argument(\"--mm_spatial_pool_stride\", type=int, default=4)\n",
    "    parser.add_argument(\"--mm_spatial_pool_out_channels\", type=int, default=1024)\n",
    "    parser.add_argument(\"--mm_spatial_pool_mode\", type=str, default=\"average\")\n",
    "    parser.add_argument(\"--image_aspect_ratio\", type=str, default=\"anyres\")\n",
    "    parser.add_argument(\"--image_grid_pinpoints\", type=str,\n",
    "                        default=\"[(224, 448), (224, 672), (224, 896), (448, 448), (448, 224), (672, 224), (896, 224)]\")\n",
    "    parser.add_argument(\"--mm_patch_merge_type\", type=str, default=\"spatial_unpad\")\n",
    "    parser.add_argument(\"--overwrite\", type=lambda x: (str(x).lower() == 'true'), default=True)\n",
    "    parser.add_argument(\"--for_get_frames_num\", type=int, default=4)\n",
    "    parser.add_argument(\"--load_8bit\", type=lambda x: (str(x).lower() == 'true'), default=False)\n",
    "    return parser.parse_args(args=args)\n",
    "\n",
    "\n",
    "def load_video(video_path, args):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0))\n",
    "    total_frame_num = len(vr)\n",
    "    # fps = round(vr.get_avg_fps())\n",
    "    # frame_idx = [i for i in range(0, len(vr), fps)]\n",
    "    uniform_sampled_frames = np.linspace(0, total_frame_num - 1, args.for_get_frames_num, dtype=int)\n",
    "    frame_idx = uniform_sampled_frames.tolist()\n",
    "    spare_frames = vr.get_batch(frame_idx).asnumpy()\n",
    "    return spare_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run inference on ActivityNet QA DataSet using the Video-ChatGPT model.\n",
    "\n",
    "Args:\n",
    "    args: Command-line arguments.\n",
    "\"\"\"\n",
    "\n",
    "args = parse_args([\n",
    "    '--model-path=lmms-lab/LLaVA-NeXT-Video-7B-DPO',\n",
    "    '--video_path=./out.mp4',\n",
    "    '--output_dir=./work_dirs/video_demo/LLaVA-NeXT-Video-7B-DPO_vicuna_v1_frames_32_stride_2',\n",
    "    '--output_name=pred',\n",
    "    '--chunk-idx=-1',\n",
    "    '--overwrite=True',\n",
    "    '--mm_spatial_pool_stride=4',\n",
    "    '--for_get_frames_num=32',\n",
    "    '--conv-mode=vicuna_v1'\n",
    "])\n",
    "\n",
    "# Initialize the model\n",
    "model_name = get_model_name_from_path(args.model_path)\n",
    "# Set model configuration parameters if they exist\n",
    "if args.overwrite == True:\n",
    "    overwrite_config = {}\n",
    "    overwrite_config[\"mm_resampler_type\"] = args.mm_resampler_type\n",
    "    overwrite_config[\"mm_spatial_pool_stride\"] = args.mm_spatial_pool_stride\n",
    "    overwrite_config[\"mm_spatial_pool_out_channels\"] = args.mm_spatial_pool_out_channels\n",
    "    overwrite_config[\"mm_spatial_pool_mode\"] = args.mm_spatial_pool_mode\n",
    "    overwrite_config[\"patchify_video_feature\"] = False\n",
    "\n",
    "    cfg_pretrained = AutoConfig.from_pretrained(args.model_path)\n",
    "\n",
    "    if \"224\" in cfg_pretrained.mm_vision_tower:\n",
    "        # suppose the length of text tokens is around 1000, from bo's report\n",
    "        least_token_number = args.for_get_frames_num * (16 // args.mm_spatial_pool_stride)**2 + 1000\n",
    "    else:\n",
    "        least_token_number = args.for_get_frames_num * (24 // args.mm_spatial_pool_stride)**2 + 1000\n",
    "\n",
    "    scaling_factor = math.ceil(least_token_number / 4096)\n",
    "    # import pdb;pdb.set_trace()\n",
    "\n",
    "    if scaling_factor >= 2:\n",
    "        if \"mistral\" not in cfg_pretrained._name_or_path.lower() and \"7b\" in cfg_pretrained._name_or_path.lower():\n",
    "            print(float(scaling_factor))\n",
    "            overwrite_config[\"rope_scaling\"] = {\"factor\": float(scaling_factor), \"type\": \"linear\"}\n",
    "        overwrite_config[\"max_sequence_length\"] = 4096 * scaling_factor\n",
    "        overwrite_config[\"tokenizer_model_max_length\"] = 4096 * scaling_factor\n",
    "\n",
    "    tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "        args.model_path, args.model_base, model_name, load_8bit=args.load_8bit, overwrite_config=overwrite_config\n",
    "    )\n",
    "else:\n",
    "    tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "        args.model_path, args.model_base, model_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./out_d20.mp4\n",
      "Time taken for inference: 2.099351167678833 seconds\n",
      "Question: A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image>\n",
      "If the given video was split into 30 equal duration segments, focusing on the feelings and atmosphere from the visuals of each segment, tell me which segment most possibly indicate a shift in atmosphere and why? Give me a short answer. ASSISTANT:\n",
      "\n",
      "Response: Segment 15.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "video_path = \"./out_d20.mp4\"\n",
    "sample_set = {}\n",
    "# system = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, polite answers to the user's questions and strictly follows the user's requirement.\"\n",
    "# question = \"Please provide a detailed description of the video, focusing on the main subjects, their actions, and the background scenes\"\n",
    "# question = \"What does this video describe? A. Buiding B.Forest C.coutryside D.Moon \\nAnswer with the option's letter from the given choices directly.\"\n",
    "# question = \"Please provide a brief description of the video, focusing on the main subjects, their actions, the background scenes, and their cultural features.\"\n",
    "# question = \"Please provide attributes about the video, focusing on the main subjects, their actions, the background scenes, and their cultural features. The formats should be like: ['jazz','soft', 'latin','leizure'].\"\n",
    "# question = \"Please provide a brief description of the video, focusing on the main subjects, their actions and facial expressions, and the background scenes. You should focus on the feelings and the atmosphere from the video.\"\n",
    "# question = \"Please provide a summary of the video, focusing on the feelings and atmosphere from the main subjects and the background scenes. Don't describe the actions of the subjects directly, but focus on the feelings and atmosphere from the video.\"\n",
    "# question = \"Refering to the lyrics of the video, provide a brief description focusing on the feelings and atmosphere from the main subjects and the background scenes. Lyrics: This is my fight song (hey) Take back my life song (hey) Prove I'm alright song (hey, ha) My power's turned on (hey) Starting right now, I'll be strong (hey) I'll play my fight song (hey) And I don't really care if nobody else believes (ha) 'Cause I've still got a lot of fight left in me.\"\n",
    "# question = \"Refering to the lyrics of the song, provide a brief description focusing on the feelings and atmosphere of the video. Lyrics: 'Like a small boat on the ocean. Sending big waves into motion. Like how a single word. Can make a heart open. I might only have one match. But I can make an explosion'\"\n",
    "# question = \"Think step by step. Refering to the lyrics of the song, provide a short single sentence summary focusing on the feelings and atmosphere of the video. Lyrics: 'Like a small boat on the ocean. Sending big waves into motion. Like how a single word. Can make a heart open. I might only have one match. But I can make an explosion'\"\n",
    "# question = \"Think step by step. Refering to the lyrics of the song, provide a short single sentence summary focusing on the feelings and atmosphere of the video. Lyrics: 'and this is crazy, but here's my number, so call me, maybe? It's hard to look right, at you baby, but here's my number, so call me, maybe? Hey, I just met you, and this is crazy, but here's my number, so call me, maybe? And all the other boys, try to chase me, but here's my number, so call me, maybe?'\"\n",
    "# question = \"Think step by step. Refering to the lyrics of the song, provide a short single sentence summary focusing on the feelings and atmosphere of the video. Lyrics: 'Like a small boat on the ocean. Sending big waves into motion. Like how a single word. Can make a heart open. I might only have one match. But I can make an explosion. And all those things I didn't say. Were wrecking balls inside my brain. I will scream them loud tonight. Can you hear my voice this time?. This is my fight song (hey). Take back my life song (hey). Prove I'm alright song (hey, ha). My power's turned on (hey). Starting right now, I'll be strong (hey). I'll play my fight song (hey). And I don't really care if nobody else believes (ha). 'Cause I've still got a lot of fight left in me. '\"\n",
    "# question = \"Think step by step. Provide a short single sentence summary focusing on the feelings and atmosphere of the video.\"\n",
    "# question = \"Watch the given video and respond to the following question: does the women first appear in the pub or in her room?\"\n",
    "question = \"If the given video was split into 30 equal duration segments, focusing on the feelings and atmosphere from the visuals of each segment, tell me which segment most possibly indicate a shift in atmosphere and why? Give me a short answer.\"\n",
    "\n",
    "\n",
    "# Check if the video exists\n",
    "if os.path.exists(video_path):\n",
    "    print(video_path)\n",
    "    video = load_video(video_path, args)\n",
    "    video = image_processor.preprocess(video, return_tensors=\"pt\")[\"pixel_values\"].half().cuda()\n",
    "    video = [video]\n",
    "\n",
    "# try:\n",
    "# Run inference on the video and add the output to the list\n",
    "\n",
    "qs = question\n",
    "if model.config.mm_use_im_start_end:\n",
    "    qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + qs\n",
    "else:\n",
    "    qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "\n",
    "conv = conv_templates[args.conv_mode].copy()\n",
    "\n",
    "# if (len(system) > 0):\n",
    "#     conv.system = system\n",
    "\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).cuda()\n",
    "attention_masks = input_ids.ne(tokenizer.pad_token_id).long().cuda()\n",
    "\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str]\n",
    "stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    start_time = time.time()\n",
    "    output_ids = model.generate(\n",
    "        inputs=input_ids,\n",
    "        images=video,\n",
    "        attention_mask=attention_masks,\n",
    "        modalities=\"video\",\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        max_new_tokens=1024,\n",
    "        use_cache=True,\n",
    "        stopping_criteria=[stopping_criteria]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken for inference: {end_time - start_time} seconds\")\n",
    "\n",
    "outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "print(f\"Question: {prompt}\\n\")\n",
    "print(f\"Response: {outputs}\\n\")\n",
    "# import pdb;pdb.set_trace()\n",
    "if outputs.endswith(stop_str):\n",
    "    outputs = outputs[: -len(stop_str)]\n",
    "outputs = outputs.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
