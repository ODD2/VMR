{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mv_dts import MusicVideoDatasetV2\n",
    "# T_RES = 10\n",
    "# dts = MusicVideoDatasetV2(\"test\", frames=10, duration=10, t_res=2)\n",
    "\n",
    "# errors = {}\n",
    "# labels = {i: 0 for i in range(T_RES)}\n",
    "\n",
    "# for i in range(len(dts)):\n",
    "#     try:\n",
    "#         b = dts[i]\n",
    "#         print(i, b[\"labels\"].item())\n",
    "#         labels[b[\"labels\"].item()] += 1\n",
    "#     except Exception as e:\n",
    "#         print(\"exception\", e)\n",
    "#         errors[i] = e\n",
    "#         continue\n",
    "# # _ = dts[random.randrange(0, len(dts))]\n",
    "# total = sum([v for k, v in labels.items()])\n",
    "# labels = {k: v / total for k, v in labels.items()}\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mv_dts import MusicVideoDatasetV3\n",
    "T_RES = 3\n",
    "dts = MusicVideoDatasetV3(\"test\", frames=20, duration=10, t_res=T_RES, with_audio=True, debug=True, offset_ratio=0.3)\n",
    "np.random.seed(1019)\n",
    "idx = 37\n",
    "print(len(dts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "b = dts.__getitem__(idx, True)\n",
    "print(b[\"labels\"].item())\n",
    "frames = b[\"frames\"].permute(2, 0, 3, 1).flatten(1, 2)\n",
    "print(frames.shape)\n",
    "plt.figure(figsize=(120, 12))\n",
    "plt.imshow(frames)\n",
    "plt.show()\n",
    "plt.close()\n",
    "idx = 1 + idx\n",
    "os.system(f\"ffmpeg -hide_banner  -loglevel error -y -ss {b['timestamps'][0]} -t {int(b['timestamps'][2]-b['timestamps'][0])} -i  {b['video_path']} sample.mp4\")\n",
    "import IPython\n",
    "IPython.display.Audio(b[\"audio\"], rate=44100, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[\"audio\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import numpy as np\n",
    "from decord import VideoReader, cpu\n",
    "\n",
    "\n",
    "def load_video(video_path):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0))\n",
    "    total_frame_num = len(vr)\n",
    "    uniform_sampled_frames = np.linspace(0, total_frame_num - 1, 15, dtype=int)\n",
    "    frame_idx = uniform_sampled_frames.tolist()\n",
    "    spare_frames = vr.get_batch(frame_idx).asnumpy()\n",
    "    return spare_frames\n",
    "\n",
    "\n",
    "def fetch_frames(video_path, duration, offset=0, num_frames=10, transform=(lambda x: x)):\n",
    "    frames = torchvision.io.read_video(video_path, offset, offset + duration, 'sec', 'TCHW')[0]\n",
    "    if (num_frames > 1):\n",
    "        stride = (frames.shape[0] - 1) / (num_frames - 1)  # number of video frames\n",
    "    else:\n",
    "        stride = 0\n",
    "    frames = frames[[int(i * stride) for i in range(num_frames)]]\n",
    "    assert frames.shape[0] == num_frames, \"number of frames is not correct\"\n",
    "    frames = transform(frames)\n",
    "    return frames\n",
    "\n",
    "\n",
    "frames_1 = load_video(\"out_2.mp4\")\n",
    "frames_2 = fetch_frames(\"out_2.mp4\", duration=30, num_frames=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in frames_1:\n",
    "    plt.imshow(i)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in frames_2.permute(0, 2, 3, 1):\n",
    "    plt.imshow(i)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor.preprocess(video, return_tensors=\"pt\")[\"pixel_values\"].half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"9\"\n",
    "from llavavid.model.builder import load_pretrained_model\n",
    "\n",
    "\n",
    "def get_model_name_from_path(model_path):\n",
    "    model_path = model_path.strip(\"/\")\n",
    "    model_paths = model_path.split(\"/\")\n",
    "    if model_paths[-1].startswith('checkpoint-'):\n",
    "        return model_paths[-2] + \"_\" + model_paths[-1]\n",
    "    else:\n",
    "        return model_paths[-1]\n",
    "\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    \"lmms-lab/LLaVA-NeXT-Video-7B-DPO\", None, get_model_name_from_path(\"lmms-lab/LLaVA-NeXT-Video-7B-DPO\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_1 = image_processor.preprocess(frames_1, return_tensors=\"pt\")[\"pixel_values\"].half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_2 = image_processor.preprocess(frames_1.transpose((0, 3, 1, 2)), return_tensors=\"pt\")[\"pixel_values\"].half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_1.shape, video_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "False in (video_1 == video_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "(frames_2.permute(0, 2, 3, 1) - torch.from_numpy(frames_1)).float().abs().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_1 == video_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "from scenedetect import detect, AdaptiveDetector, split_video_ffmpeg\n",
    "SCENE_CUT_DIR = \"scene_cut\"\n",
    "FRAME_CUT_DIR = \"frame_cut\"\n",
    "\n",
    "scene_list = detect(\"/scratch4/users/od/YTCharts/videos/-KKMiVcuwJk.mp4\", AdaptiveDetector())\n",
    "# scene_list = detect('/scratch4/users/od/YTCharts/v1/test/--Sh1ABUSas.mp4', AdaptiveDetector())\n",
    "len(scene_list)\n",
    "# save each scene cut\n",
    "# split_video_ffmpeg('/scratch4/users/od/YTCharts/v1/test/6R4JLBC3vqc.mp4', scene_list, output_dir=SCENE_CUT_DIR)\n",
    "\n",
    "\n",
    "# MIN_CUTS = 15\n",
    "# cut_frame_indices = []\n",
    "# if (len(scene_list) >= MIN_CUTS):\n",
    "#     for (a, b) in scene_list:\n",
    "#         cut_frame_indices.append((a.frame_num + b.frame_num) // 2)\n",
    "# else:\n",
    "#     # count ratio\n",
    "#     per_scene_frame_num = []\n",
    "#     for (a, b) in scene_list:\n",
    "#         per_scene_frame_num.append(b.frame_num - a.frame_num)\n",
    "#     total = sum(per_scene_frame_num)\n",
    "#     ratio = [\n",
    "#         max(1, round(MIN_CUTS * (frame_num / total)))\n",
    "#         for frame_num in per_scene_frame_num\n",
    "#     ]\n",
    "#     ratio[-1] += max(MIN_CUTS - sum(ratio), 0)\n",
    "\n",
    "#     for r, f, (a, b) in zip(ratio, per_scene_frame_num, scene_list):\n",
    "#         hop = f // (r + 1)\n",
    "#         for i in range(r):\n",
    "#             cut_frame_indices.append(a.frame_num + (1 + i) * hop)\n",
    "\n",
    "# print(cut_frame_indices)\n",
    "\n",
    "# os.makedirs(FRAME_CUT_DIR, exist_ok=True)\n",
    "# cap = cv2.VideoCapture(\"./original.mp4\")\n",
    "# j = 0\n",
    "# for i in range(round(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
    "#     ret, frame = cap.read()\n",
    "#     if (i in cut_frame_indices):\n",
    "#         cv2.imwrite(os.path.join(FRAME_CUT_DIR, f\"{j}.png\"), frame)\n",
    "#         j += 1\n",
    "# cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "lr_sched = torch.optim.lr_scheduler.OneCycleLR(torch.optim.AdamW([torch.tensor([0, 0, 0])], lr=0.1), 1e-4, total_steps=100, epochs=None, steps_per_epoch=None, pct_start=0.3, anneal_strategy='cos',\n",
    "                                               cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, three_phase=False, last_epoch=-1, verbose='deprecated')\n",
    "\n",
    "lr = []\n",
    "for i in range(100):\n",
    "    lr_sched.step()\n",
    "    lr.append(lr_sched.get_last_lr())\n",
    "\n",
    "plt.plot(range(100), lr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scenes_list = []\n",
    "for file in glob(\"/scratch4/users/od/YTCharts/v1/*/*.json\"):\n",
    "    with open(file, \"r\") as f:\n",
    "        scenes = json.load(f)[\"scenes\"]\n",
    "        if (scenes == 10):\n",
    "            print(file)\n",
    "        scenes_list.append(scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins = np.histogram(scenes_list, bins=100, range=(0, 300))\n",
    "plt.stairs(counts, bins)\n",
    "plt.xlabel(\"scene cuts\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"YTCharts Scene Cut Histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch.utils\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from transformers import AutoConfig\n",
    "from peft import LoraConfig, TaskType, PeftModel, get_peft_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from llavavid.conversation import conv_templates, SeparatorStyle\n",
    "from llavavid.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n",
    "from llavavid.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "\n",
    "\n",
    "def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\"auto\", overwrite_config=None):\n",
    "    import os\n",
    "    import torch\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
    "    from llavavid.model import LlavaLlamaForCausalLM\n",
    "    from llavavid.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "    kwargs = {\"device_map\": device_map}\n",
    "\n",
    "    # import pdb;pdb.set_trace()\n",
    "    if load_8bit:\n",
    "        kwargs[\"load_in_8bit\"] = True\n",
    "    elif load_4bit:\n",
    "        kwargs[\"load_in_4bit\"] = True\n",
    "        kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n",
    "    else:\n",
    "        kwargs[\"torch_dtype\"] = torch.float16\n",
    "\n",
    "  # this may be mm projector only\n",
    "    print(\"Loading LLaVA from base model...\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "    cfg_pretrained = AutoConfig.from_pretrained(model_path)\n",
    "    if overwrite_config is not None:\n",
    "        print(f\"Overwriting config with {overwrite_config}\")\n",
    "        for k, v in overwrite_config.items():\n",
    "            setattr(cfg_pretrained, k, v)\n",
    "    print(cfg_pretrained)\n",
    "    model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\n",
    "\n",
    "    image_processor = None\n",
    "\n",
    "    assert \"llava\" in model_name.lower(), \"Only LLaVA models are supported for video chatbot.\"\n",
    "    mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n",
    "    mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\n",
    "    if mm_use_im_patch_token:\n",
    "        tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n",
    "    if mm_use_im_start_end:\n",
    "        tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    vision_tower = model.get_vision_tower()\n",
    "    if not vision_tower.is_loaded:\n",
    "        vision_tower.load_model(device_map=device_map)\n",
    "    vision_tower.to(device=model.device, dtype=model.dtype)\n",
    "    image_processor = vision_tower.image_processor\n",
    "\n",
    "    if hasattr(model.config, \"max_sequence_length\"):\n",
    "        context_len = model.config.max_sequence_length\n",
    "    else:\n",
    "        context_len = 2048\n",
    "\n",
    "    return tokenizer, model, image_processor, context_len\n",
    "\n",
    "\n",
    "CONV_MODE = \"vicuna_v1\"\n",
    "MODEL_PATH = \"lmms-lab/LLaVA-NeXT-Video-7B-DPO\"\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model_name = get_model_name_from_path(MODEL_PATH)\n",
    "\n",
    "# Set model configuration parameters if they exist\n",
    "overwrite_config = {}\n",
    "overwrite_config[\"mm_resampler_type\"] = \"spatial_pool\"\n",
    "overwrite_config[\"mm_spatial_pool_stride\"] = 2\n",
    "overwrite_config[\"mm_spatial_pool_out_channels\"] = 1024\n",
    "overwrite_config[\"mm_spatial_pool_mode\"] = \"average\"\n",
    "overwrite_config[\"patchify_video_feature\"] = False\n",
    "\n",
    "cfg_pretrained = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "\n",
    "if \"224\" in cfg_pretrained.mm_vision_tower:\n",
    "    # suppose the length of text tokens is around 1000, from bo's report\n",
    "    least_token_number = 10 * (16 // 2)**2 + 1000\n",
    "else:\n",
    "    least_token_number = 10 * (24 // 2)**2 + 1000\n",
    "\n",
    "scaling_factor = math.ceil(least_token_number / 4096)\n",
    "\n",
    "if scaling_factor >= 2:\n",
    "    if \"mistral\" not in cfg_pretrained._name_or_path.lower() and \"7b\" in cfg_pretrained._name_or_path.lower():\n",
    "        print(float(scaling_factor))\n",
    "        overwrite_config[\"rope_scaling\"] = {\"factor\": float(scaling_factor), \"type\": \"linear\"}\n",
    "    overwrite_config[\"max_sequence_length\"] = 4096 * scaling_factor\n",
    "    overwrite_config[\"tokenizer_model_max_length\"] = 4096 * scaling_factor\n",
    "\n",
    "# Load model with new configuration\n",
    "_, model, _, _ = load_pretrained_model(\n",
    "    MODEL_PATH,\n",
    "    None,\n",
    "    model_name,\n",
    "    False,\n",
    "    overwrite_config=overwrite_config,\n",
    "    device_map=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch.nn.init.kaiming_uniform_ = lambda x, *args, **kwargs: x\n",
    "# torch.nn.init.uniform_ = lambda x, *args, **kwargs: x\n",
    "with torch.device(\"meta\"):\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        \"logs/0729_164405/v4_e47b624_loss=0.87\",\n",
    "        is_trainable=False,\n",
    "        low_cpu_mem_usage=True,\n",
    "        init_lora_weights=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "with open(\"data.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = data[\"eval_labels\"]\n",
    "plt.hist(labels, bins=3, weights=np.ones(len(labels)) / len(labels))\n",
    "plt.show()\n",
    "plt.close()\n",
    "probs = data[\"eval_probs\"]\n",
    "preds = probs.argmax(axis=-1)\n",
    "plt.hist(preds, bins=3, weights=np.ones(len(preds)) / len(preds))\n",
    "plt.show()\n",
    "plt.close()\n",
    "print(\"Accuracy:\", (labels == preds).astype(int).mean())\n",
    "print(\"Loss:\", -np.log(probs[range(len(labels)), labels] + 1e-4).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "with open(\"data.pickle\", \"rb\") as f:\n",
    "    in_data = pickle.load(f)[\"eval_records\"]\n",
    "\n",
    "\n",
    "tr_data = torch.load(\"logs/0802_044652/v4_e4b0_eval/aux.pt\")[\"eval_records\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = sorted(in_data, key=lambda x: x[\"idx\"])\n",
    "tr_data = sorted(tr_data, key=lambda x: x[\"idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in zip(in_data, tr_data):\n",
    "    if (not a[\"video_path\"] == b[\"video_path\"]):\n",
    "        print(\"Video Path\", a[\"video_path\"], b[\"video_path\"])\n",
    "    if (not a[\"idx\"] == b[\"idx\"]):\n",
    "        print(\"Video IDX:\", a[\"idx\"], b[\"idx\"])\n",
    "    if (not ((a[\"timestamps\"] - b[\"timestamps\"]).abs().sum() == 0)):\n",
    "        print(\"TimeStamps:\", a[\"timestamps\"], b[\"timestamps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "aux = torch.load(\"logs/0802_044652/v4_e4b0_eval/aux.pt\")\n",
    "adapt = torch.load(\"logs/0802_044652/v4_e4b0_eval/adapter_model.bin\")\n",
    "out = torch.load(\"logs/0802_044652/v4_e4b0_eval/out.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    ((aux[\"t_proj\"][\"proj3.weight\"].cpu() - out[\"t_proj.proj3.weight\"].cpu().half())).abs().float().sum(),\n",
    "    ((aux[\"embed_tokens\"][\"weight\"].cpu() - out[\"model.base_model.model.model.embed_tokens.weight\"].cpu().half())).abs().float().sum(),\n",
    "    ((aux[\"lm_head\"][\"weight\"].cpu() - out[\"model.base_model.model.lm_head.weight\"].cpu().half())).abs().float().sum(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([\n",
    "    ((adapt[f\"base_model.model.model.layers.{i}.self_attn.{v}_proj.lora_{t}.weight\"].cpu() -\n",
    "     out[f\"model.base_model.model.model.layers.{i}.self_attn.{v}_proj.lora_{t}.default.weight\"].cpu().half())).abs().float().sum()\n",
    "    for i in range(32)\n",
    "    for t in [\"A\", \"B\"]\n",
    "    for v in [\"q\", \"k\", \"v\", \"o\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([\n",
    "    ((adapt[f\"base_model.model.model.layers.{i}.mlp.{v}_proj.lora_{t}.weight\"].cpu() -\n",
    "     out[f\"model.base_model.model.model.layers.{i}.mlp.{v}_proj.lora_{t}.default.weight\"].cpu().half())).abs().float().sum()\n",
    "    for i in range(32)\n",
    "    for t in [\"A\", \"B\"]\n",
    "    for v in [\"gate\", \"down\", \"up\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
